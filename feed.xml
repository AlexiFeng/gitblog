<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom"><id>https://github.com/AlexiFeng/gitblog</id><title>RSS feed of AlexiFeng's gitblog</title><updated>2023-06-25T18:26:12.544812+00:00</updated><author><name>AlexiFeng</name><email>654973939@qq.com</email></author><link href="https://github.com/AlexiFeng/gitblog"/><link href="https://raw.githubusercontent.com/AlexiFeng/gitblog/master/feed.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator><entry><id>https://github.com/AlexiFeng/gitblog/issues/15</id><title>I'm back.</title><updated>2023-06-25T18:26:12.976094+00:00</updated><content type="html"><![CDATA[<p>我摸鱼大概摸了一个月？差不多。这怎么都6月26日了，我什么时候从大连回深圳的我都没印象了。感觉自己好像断片了一个月一样。
7月10日左右好像还有中期，我是不是得准备一下。</p>
<p>这一个月除了玩游戏什么也没做，还有完全的拒绝沟通，拒绝和所有人沟通，所有人，我爸我妈，导师，组里的人，所有所有的人。
我不是故意的，在我内心好像我做错了什么事情一样，我不敢去面对任何人。对不起。</p>
<p>Join the black parade alone.
<img src="https://github.com/AlexiFeng/gitblog/assets/16517113/bcccc473-9fd6-4225-a382-de4645a3f658" alt="image" /></p>
<hr />
<p>我突然回想起我上高中的事情，其实我语文成绩突然提高并不一定全是宋词的功劳。还有一个原因是高一的时候我突然开始写日记和qq空间的日志，写很多。说实话那个文笔我看着比我现在好多了，我现在写的啥破流水账。
反正就是综合原因提升的水平吧。</p>
<hr />
<p>我现在戴着森海的二代大馒头听nightwish的歌，听的是Nemo.感觉还是这个耳机听歌有感觉，虽然戴久了耳朵疼。第一次接触nightwish还是甜咖啡制作的cs的mv。
梦回小学了这波是hhh</p>
<hr />
<p>我发现一个非常准的评测我是否在状态的指标，就是我不在状态的时候不会听歌。那段时间（几天，几周）都不会听。
一旦我开始想正经的事，我就会开始听歌。</p>
<hr />
<p>这段时间喜欢在河边坐着，这个坐着的行为与沉迷玩游戏呈现了很明显的反差，说明我在尝试逃离目前的这种断片的状态。</p>
<p>在河边坐着也引申出来了夜跑的想法。上周我的想法是能跑则跑，跑不了走一走也是好的。
这周开始我决定制定目标，比如说穿着我的rider25，这周总共跑10km。</p>
<blockquote>
<p>想想高中和大学的自己从来不关注穿的什么鞋，笑死。也是那个时候没胖，光脚跑都没问题。</p>
</blockquote>
<p>今晚第一次跑，我得承认我现在是真的跑不动，我跑一公里都费老劲了。今天死活才凑出来2.5km。深刻体会到了什么叫：我胖了，但是我小腿没胖。它撑着以前的我还差不多，现在它真的撑不住啊。
没事，我相信只要开始就会逐渐进步，贵在坚持。而且我虽然跑不动，但好消息是我这两次跑的挺开心的，有点回到之前的感觉。而且每次一开始跑我就会思考人生，嗯，我从小就这样。</p>
<hr />
<blockquote>
<p>I'm in love with my lust,
burning angel wings to dust.
——Wish I had an angel  ,Nightwish</p>
</blockquote>
<p>这句歌词我可太熟悉了。</p>
<hr />
<p>我这是意识流写作？感觉内容怎么残缺不堪的。</p>
<hr />
<p>没事，只要开始做怎么都不晚，对吧。我觉得我还是挺有信心的。</p>
]]></content><link href="https://github.com/AlexiFeng/gitblog/issues/15" rel="alternate"/><category term="吐槽"/><published>2023-06-25T18:25:39+00:00</published></entry><entry><id>https://github.com/AlexiFeng/gitblog/issues/14</id><title>raw去噪终于做完了！！！！</title><updated>2023-06-25T18:26:13.173594+00:00</updated><content type="html"><![CDATA[<p>我得承认raw去噪这个项目给我快做秃了，前期主要是熟悉项目及流程还好。后面的话完全是在虚空索敌。
甲方的结果是基于多帧图像进行去噪的，而我是单帧图像去噪。这就意味着在高噪声环境下我就不可能比他效果好。举个极端例子。
<img src="https://github.com/AlexiFeng/gitblog/assets/16517113/28311aec-7a3e-4073-9ddd-5f2e73014324" alt="image" />
这是我的原图像。但这用多帧去噪的结果呢？
<img src="https://github.com/AlexiFeng/gitblog/assets/16517113/7cd6fb42-6eb3-4a62-adb8-87bba8d67c8c" alt="image" />
我这辈子不可能做出来这个效果😭这已经不是去噪任务了，是diffusion了，我给你画一个草出来吧。
这就是我这段时间在进行的虚空索敌，一直在尽可能优化我的效果，但我根本不知道什么时候是个头。我跟导师说过不止一次，我能保证看得到的细节我都比对方好。但是我没法评价一张图片，因为有些已经被高噪盖住的信息我不可能给他恢复出来。
这段时间真的搞的很累，肉眼可见的憔悴。总算是大概差不多终于结束了。
okk</p>
]]></content><link href="https://github.com/AlexiFeng/gitblog/issues/14" rel="alternate"/><category term="吐槽"/><published>2023-05-10T05:29:15+00:00</published></entry><entry><id>https://github.com/AlexiFeng/gitblog/issues/13</id><title>how to use pytorch to train model with DistributedDataParallel </title><updated>2023-06-25T18:26:13.361813+00:00</updated><content type="html"><![CDATA[<p>Many nouns are also used in course &quot;Parallel Computing&quot;</p>
<pre><code class="language-python">parser = argparse.ArgumentParser()
parser.add_argument(&quot;--local_rank&quot;, default=-1)
FLAGS = parser.parse_args()
local_rank = int(FLAGS.local_rank)

# 新增3：DDP backend初始化
#   a.根据local_rank来设定当前使用哪块GPU
torch.cuda.set_device(local_rank)
#   b.初始化DDP，使用默认backend(nccl)就行。如果是CPU模型运行，需要选择其他后端。
dist.init_process_group(backend=&#x27;nccl&#x27;)
device = torch.device(&quot;cuda&quot;, local_rank)
model=SimpleNet().to(device) #init model
model = DDP(model, device_ids=[local_rank], output_device=local_rank)  #use DDP
</code></pre>
<p>and then,must use distributedsampler,distribute different data to each process.</p>
<pre><code class="language-python">train_sampler  = torch.utils.data.distributed.DistributedSampler(train_dataset)
</code></pre>
<p>use one process to save model</p>
<pre><code class="language-python">if dist.get_rank()==0:
    meg.save(model.module, save_path+str(cur_epoch)+ &quot;.pth&quot;)
</code></pre>
<p>then should use barrier?I've no idea.</p>
]]></content><link href="https://github.com/AlexiFeng/gitblog/issues/13" rel="alternate"/><category term="TODO"/><category term="python"/><published>2023-05-04T15:52:59+00:00</published></entry><entry><id>https://github.com/AlexiFeng/gitblog/issues/12</id><title>pytorch的多卡训练的模型在测试的时候不能直接运行</title><updated>2023-06-25T18:26:13.553951+00:00</updated><content type="html"><![CDATA[<p>这个坑遇到过两次了，多卡联合训练的时候模型直接存储会多一个module。很多时候用dataparallel测试不太现实。
解决办法1：</p>
<pre><code class="language-python"># save model
if num_gpu ==  1:
    torch.save(model.module.state_dict(), &#x27;net.pth&#x27;)
 else:
    torch.save(model.state_dict(),  &#x27;net.pth&#x27;)
</code></pre>
<p>办法2：
把训练好的模型里的model字符删除(我目前用的主要是这种）反正也不麻烦。</p>
<pre><code class="language-python">pth = torch.load(&#x27;./626.pth&#x27;)
from collections import OrderedDict
new_state_dict = OrderedDict()
for k, v in pth.items():
    name =  k[7:] # remove  &#x27;module&#x27;
    new_state_dict[name]=v
model.load_state_dict(new_state_dict)
model.eval()
</code></pre>
<blockquote>
<p>原文链接：<a href="https://blog.csdn.net/szn1316159505/article/details/129225188">https://blog.csdn.net/szn1316159505/article/details/129225188</a></p>
</blockquote>
]]></content><link href="https://github.com/AlexiFeng/gitblog/issues/12" rel="alternate"/><category term="python"/><published>2023-04-23T07:48:37+00:00</published></entry><entry><id>https://github.com/AlexiFeng/gitblog/issues/11</id><title>[Raw域去噪]ISP的流程及raw图片处理</title><updated>2023-06-25T18:26:13.747785+00:00</updated><content type="html"><![CDATA[<p>去年年底的时候做了一个AI-ISP的项目，我负责的是raw域的去噪。与我们常见的ai算法不同，它是直接作用于相机原始数据的，更偏向底层数据处理，这也让我在做的过程中也对ISP部分有了一点了解。在这里写下一个入门笔记，仅供参考。</p>
<hr />
<p>从宏观角度来讲，我们看到的照片通常经历了如下一个流程：光通过镜头打到CMOS传感器上-》得到原始数据（Bayer Mosaic）-》Demosaic-》白平衡，Gamma校正...-》出图</p>
<h2>光打到CMOS传感器上</h2>
<p>这个地方我特意提到了光打到CMOS传感器上，因为这是radiation noise形成的主要原因，而这与我后面要做的项目密切相关。</p>
<h2>Bayer Raw</h2>
<p>raw格式文件通常是照片的原始文件，它是Bayer Mosaic的，可以简单理解为它并不是以RGB格式存储的，而是以RGGB/BGGR/RGBG...等方式进行存储的。当拍摄彩色图像的时候，最朴素的采样逻辑是用多块单色滤镜（红绿蓝三原色）拍摄单色图像，然后组合成一副彩色图像。但是这样造假高，而且必须要保证滤镜的位置完全对齐，不然一旦有像素偏差就会出现重影。Bayer则只用了一块滤镜，在不同位置设置不同的颜色，<strong>由于人眼对绿色比较敏感，所以绿色用的更多</strong>（这也解释了为什么Bayer Raw的格式里面都有两个Green,同时解释了demosaic之后的图片是绿色的）。
RGGB这只是滤色矩阵的编码方式，指的是一个2*2的像素方阵里各个颜色的顺序。直接附上一张图比较清晰。
<img src="https://user-images.githubusercontent.com/16517113/232278729-dc7dbfb2-b489-4c11-b538-50afc541ff0a.png" alt="image" /></p>
<h2>Demosaic</h2>
<p>为了将Bayer Raw重建为我们熟悉的RGB图像，需要对图像中的每一个像素点进行插值, 利用其周围像素点的色彩值来估计出缺失的另外两个色彩值, 最终得到一个每个像素点包含红、绿、蓝三个像素值的全彩色图, 这个过程就叫做Demosaic。如果使用了与滤色矩阵不适配的demosaic方法将会转出很奇怪的结果，轻则颜色错误，重则全是马赛克。opencv里有demosaic算法可以直接调用。这个阶段之后，图片就成为我们平时场景的格式了。在这个基础上再进行awb等操作。</p>
<p>对于正常相机拍出来的raw图片，通常可以使用rawpy进行直接读取并进行后处理，直接得到全彩色图像。而对于那种没有数据头的纯数据图像，可以使用numpy进行读取，然后用opencv做demosaic得到全彩色图像，然后再另寻办法做awb等后处理操作。关于raw格式图片的读取，通常有raw8，raw10，raw12，raw14，raw16这些格式。其实就是每一个像素值占多少位。而这里存在一些问题，比如用numpy进行文件读取的时候只有uint8和uint16，对于其他格式没法直接读取。我自己测试的结果是12位可以用uint16直接读，但是10位就会出问题。虽然raw的位数有很多种，但是处理起来其实很简单。最简单的处理方法就是直接扩大位数，以raw8转raw10为例，只需要对所有数据*4（也就是2的2次方）即可。而10位转8位只需要/4就可以。这样可以将图片全都转到raw8来处理，当然其他格式之间也可以互相转。raw8是最常用的，因为opencv显示图片之类的方法默认只支持8位图像。其实这个操作我觉得很像normalize之后再还原回去。</p>
<p><strong>有一个坑就是如果使用m1版的conda是装不了rawpy的，pip会提示找不到。这不是你的问题，我扫了一眼github貌似是不推荐在m1版上使用。</strong></p>
<p>关于使用numpy+opencv处理图像以及raw格式转换附上一个参考代码</p>
<pre><code class="language-python">import numpy as np
def raw_convert(raw,source,target):
    return raw*np.power(2,float(target-source)).astype(np.uint16)
def read_raw(file_name,shape):
    # 从raw文件中读取数据
    data = np.fromfile(file,dtype=&quot;uint16&quot;)
    data.resize(shape)
    return data
def raw2png(raw,path)
    dst=cv2.cvtColor(raw.astype(&quot;uint8&quot;), cv2.COLOR_BayerBG2BGR)#我之前看文档，这一步应该直接就做demosaic了
    cv2.imwrite(str(path), dst)
</code></pre>
<h2>Bayer格式之间互转</h2>
<p>参考这个库https://github.com/Jiaming-Liu/BayerUnifyAug
**由于算法的关系（具体可以看仓库指向的论文）Bayer互转后分辨率会稍有变化，当你在numpy resize的时候格式不再是原来分辨率。可以在转完之后直接print一下现在的shape</p>
]]></content><link href="https://github.com/AlexiFeng/gitblog/issues/11" rel="alternate"/><category term="Image"/><published>2023-04-15T18:54:36+00:00</published></entry><entry><id>https://github.com/AlexiFeng/gitblog/issues/10</id><title>记录一些我觉得很厉害的人</title><updated>2023-06-25T18:26:13.941420+00:00</updated><content type="html"><![CDATA[<p><a href="https://blog.csdn.net/hanss2">https://blog.csdn.net/hanss2</a>
之前在搜401毕业论文Latex模板的时候搜到的这个人。震惊于其的学习效率及博客产出量大质量高。</p>
<p><a href="https://github.com/yihong0618/gitblog">https://github.com/yihong0618/gitblog</a>
该博客的原作者，同样觉得他的博客更新的很勤。都是很自律的dalao。</p>
]]></content><link href="https://github.com/AlexiFeng/gitblog/issues/10" rel="alternate"/><published>2023-04-15T05:46:18+00:00</published></entry></feed>